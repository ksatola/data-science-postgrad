{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = zeros(labels.shape[0], np.max(labels) + 1) \n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot.astype(np.float32)\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(batch):\n",
    "    return np.maximum(0.0, batch)\n",
    "\n",
    "def relu_derivative(batch):\n",
    "    # Zaimplementuj pochodną funkcji aktywacji ReLU\n",
    "    #\n",
    "    # return ???\n",
    "    raise NotImplementedError('Pochodna ReLU jest nie zaimplementowana')\n",
    "\n",
    "def softmax(batch):\n",
    "    # Zaimplementuj funkcję aktywacji Softmax\n",
    "    #\n",
    "    # return ???\n",
    "    raise NotImplementedError('Funkcja aktywacji Softmax jest nie zaimplementowana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zbiór danych MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warstwy sieci neuronowej\n",
    "\n",
    "Budujemy sieci złożone z dwóch rodzajów warstwy: warstwy w pełni połączonej (```Dense```) i warstwy, które wylicza nieliniową funkcję aktywacji (```Nonlinear```). Obie warstwy implementują dwie funkcje\n",
    "\n",
    "- propagacja sygnału: ```forward```\n",
    "- wsteczna propagacja błędu: ```backward```\n",
    "\n",
    "Operacja ```backward``` propaguje gradient funkcji kosztu do poprzedniej warstwy. Dodatkowo, w przypadku warstwy w pełni połączonej wylicza pochodne funkcji kosztu po wagach i składowej stałej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warstwa nieliniowej funkcji aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nonlinear:\n",
    "    def __init__(self, activation_fun, d_activation_fun):\n",
    "        self.activation_fun = activation_fun\n",
    "        self.d_activation_fun = d_activation_fun\n",
    "        \n",
    "        self.visible = None\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.visible = batch\n",
    "        return self.activation_fun(batch)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # Zaimplementuj wsteczną propagację błędu dla nieliniowej funkcji aktywacji\n",
    "        #\n",
    "        # return ???\n",
    "        raise NotImplementedError('Wsteczna propagacja błędu jest nie zaimplementowana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warstwa w pełni połączona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, visible_size, hidden_size, learning_rate, momentum):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=np.sqrt(2.0 / self.visible_size),\n",
    "                                  size=(self.visible_size, self.hidden_size)).astype(np.float32)\n",
    "        self.B = zeros(1, self.hidden_size)\n",
    "        \n",
    "        self.visible = None\n",
    "\n",
    "        self.MW = zeros(self.visible_size, self.hidden_size)\n",
    "        self.MB = zeros(1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.visible = batch\n",
    "        \n",
    "        # Zaimplementuj propagację sygnału dla warstwy w pełni połączonej\n",
    "        #\n",
    "        raise NotImplementedError('Propagacja sygnału jest nie zaimplementowana')\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        observations_count = self.visible.shape[0]\n",
    "        \n",
    "        prev_grad = grad @ self.W.T\n",
    "        \n",
    "        grad_w = self.visible.T @ grad / observations_count\n",
    "        grad_b = np.sum(grad, axis=0) / observations_count\n",
    "        \n",
    "        self.W -= self.learning_rate * grad_w\n",
    "        self.B -= self.learning_rate * grad_b\n",
    "        \n",
    "        return prev_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wsteczna propagacja błędu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(mlp, batch):\n",
    "    for layer in mlp:\n",
    "        batch = layer.forward(batch)\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_backpropagate(mlp, grad):\n",
    "    for layer in reversed(mlp):\n",
    "        grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mlp(mlp, dataset, labels, batch_size):\n",
    "    batches_limit = dataset.shape[0] // batch_size\n",
    "    \n",
    "    batched_data = chunks(dataset, batch_size)\n",
    "    batched_labels = chunks(labels, batch_size)\n",
    "    \n",
    "    for batch_idx, (batch, batch_labels) in enumerate(zip(batched_data, batched_labels)):\n",
    "\n",
    "        y = forward_pass(mlp, batch)\n",
    "        \n",
    "        grad = y - batch_labels\n",
    "        error_backpropagate(mlp[0:-1], grad)\n",
    "        \n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja cyfr MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(mlp, batch):\n",
    "    probabilities = np.concatenate([forward_pass(mlp, mini_batch) for mini_batch in chunks(batch, 128)])\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def run_training(mlp, train_set, train_labels,\n",
    "                 validation_set, validation_labels,\n",
    "                 batch_size, epochs_count):\n",
    "    \n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoka {}:\".format(epoch+1),  end=\"\\t\")\n",
    "\n",
    "        if epoch == 5:\n",
    "            for layer in mlp:\n",
    "                layer.momentum = 0.9\n",
    "                \n",
    "        start_time = time.time()\n",
    "        train_mlp(mlp, train_set, train_labels, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        predictions = classify(mlp, validation_set)\n",
    "        accuracy = 100.0 * np.sum(predictions == validation_labels) / predictions.shape[0]\n",
    "        print(\"\\tczas: {0:>2.2f}s, dokładność: {1:>2.2f}\".format(elapsed, accuracy))\n",
    "\n",
    "    print(\"Trening zakończony!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "DATASET_SIZE = 10000 # 60000 dla całego zbioru danych\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "##### Zbiór uczący #####\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = mnist_train_images[:DATASET_SIZE]\n",
    "mnist_train_images = mnist_train_images.reshape(-1, DIGIT_SIZE * DIGIT_SIZE)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "##### Zbiór testowy #####\n",
    "\n",
    "mnist_test_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_test_images = mnist_test_images.reshape(-1, DIGIT_SIZE * DIGIT_SIZE)\n",
    "\n",
    "mnist_test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "mlp = [\n",
    "    Dense(\n",
    "        visible_size = 28 * 28,\n",
    "        hidden_size = 256,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Dense(\n",
    "        visible_size = 256,\n",
    "        hidden_size = 256,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Dense(\n",
    "        visible_size = 256,\n",
    "        hidden_size = 256,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Dense(\n",
    "        visible_size = 256,\n",
    "        hidden_size = 10,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(softmax, None)\n",
    "]\n",
    "\n",
    "run_training(mlp,\n",
    "             mnist_train_images, mnist_train_labels,\n",
    "             mnist_test_images, mnist_test_labels,\n",
    "             BATCH_SIZE, EPOCHS_COUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
